# test_cuda.py
"""
ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ CUDA ÎºÎ±Î¹ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ GPU cache
"""

import torch
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_cuda():
    """
    ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ CUDA Î´Î¹Î±Î¸ÎµÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î±Ï‚ ÎºÎ±Î¹ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¹ÏÎ½ GPU
    """
    try:
        # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ Cleared GPU cache")

        # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ CUDA
        logger.info(f"ğŸ” CUDA available: {torch.cuda.is_available()}")

        if torch.cuda.is_available():
            logger.info(f"ğŸ“Š CUDA device: {torch.cuda.get_device_name(0)}")
            logger.info(
                f"ğŸ’¾ GPU memory allocated: {torch.cuda.memory_allocated(0)/1024**2:.1f} MB"
            )
            logger.info(
                f"ğŸ’¾ GPU memory cached: {torch.cuda.memory_reserved(0)/1024**2:.1f} MB"
            )

    except Exception as e:
        logger.error(f"âŒ Error during CUDA test: {e}")
        raise


if __name__ == "__main__":
    test_cuda()
